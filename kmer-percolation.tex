% NOTES:
%%    do we want to explore for larger S?  Does false positive rate change?

% review: first round, chris, enbody, cole/jordan, chris hart
% second round yanni, curt, james foster, abdol.

% TEST

\documentclass[12pt]{article} \usepackage{simplemargins}
\usepackage[pdftex]{graphicx} \graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

\begin{document}

\title{A probabilistic framework for compressible assembly graphs}
\author{Jason Pell, Arend Hintze, Rosangela Canino-Koning, C. Titus Brown}

\maketitle

\section{Abstract}

The memory requirements for de novo assembly of shotgun sequencing
data from metagenome and transcriptome samples are an increasingly
large practical barrier to next-generation sequence analysis.  Here we
introduce a novel probabilistic graph representation for de Bruijn
graphs that is based on Bloom filters.  This representation is
extremely memory efficient, enabling storage of de Bruijn graphs with
6 bits per node.  While the graph contains false nodes and
edges due to the underlying probabilistic representation, we show that
it can be used to assess global graph structure with high accuracy
until a specific false positive rate is reached.  We demonstrate the
utility of this representation for graph partitioning, an approach
that decomposes de Bruijn graphs into disconnected components in order
to decrease memory and compute requirements for sequence assembly.

%% @@CTB do we show 6 bits/node??

\section{Introduction}

De novo assembly of shotgun sequencing data sets is extremely memory
intensive, and it is now easy to generate more sequencing data than
can be handled on commodity computing hardware.  This is especially
true in metagenomics, where assembling shotgun sequences
from complex microbial samples required 512 GB or more of 
RAM\cite{pmid20203603, pmid21273488}.  Assembling transcriptome data from non-model
organisms also requires substantial amounts of memory, e.g. 1 GB of
RAM per million reads\cite{trinity}.  These substantial memory requirements for
assembling large data sets are increasingly becoming an obstacle to
sequence analysis in metagenomics and transcriptomics (refs).

The predominant assembly formalism applied to short-read sequencing
data sets from Illumina, SOLiD, and Roche 454 machines is a de Bruijn
graph.  In a de Bruijn graph approach, sequencing reads are decomposed
into fixed-length words, or k-mers, and used to build a connectivity
graph.  This graph is then traversed to determine contiguous sequences
made from overlapping k-mers.  Because de Bruijn graphs store only
k-mers, memory usage scales with the number of unique k-mers in the
data set rather than the number of reads.  Thus human genomes can be
assembled in as little as 512 GB of RAM\cite{pmid21187386}.  For
more complex samples such as soil metagenomes, which may possess a
million or more species, terabytes of memory are required to store the
graph.  With mRNA samples that require deep sequencing to sample rare
molecules, novel k-mers introduced by sequencing error may result in
similarly large memory requirements.

One approach to de novo assembly of metagenomes and transcriptomes is
to apply graph partitioning, in which the de Bruijn graph is divided
up into largely disconnected components\cite{trinity, metavelvet} (+ metaIDB). 
While the initial partitioning step is as memory
intensive as assembly, once the graph is partitioned the components
can be assembled individually with parameters chosen for each
component (cite).  This results in improved assemblies, but does not
address memory scaling.

To apply partitioning to memory scaling, we here describe a novel
probabilistic representation for storing de Bruijn graphs in memory,
based on Bloom filters\cite{bloom}.  Bloom filters are constant-memory
probabilistic data structures for storing sets; essentially hash
tables without collision detection, they can yield false positives on
set membership queries, but not false negatives.  We show that this
probabilistic graph representation can be used to efficiently store
and traverse DNA de Bruijn graphs with an approximately 20-fold
decrease in memory usage over the Velvet assembler. Moreover, this
graph representation can be used to accurately partition assembly
graphs.  We relate changes in local and global garph connectivity
to the false positive rate of the underlying Bloom filters, and
show that the graph's global structure is accurate until an abrupt
transition in false connectivity near a false positive rate of 0.183,
corresponding to a lower memory limit of 6 bits per graph node.

% @@ figure for how we're storing DBGs in a Bloom filter.
% @@ exact/inexact data influence

\section{Methods}

\subsection{Genome and Sequence Data}
We used the \emph{E. coli} K-12 MG1655 genome (GenBank: U00096.2) and two MG1655 Illumina 
datasets (Short Read Archive accessions SRX000429 and SRX000430) for some
analyses. 

\subsection{Data Structure Implementation}
We implemented a variation on the Bloom filter data structure to store
k-mers in memory. In a classic Bloom filter, multiple hash functions
map into a single hash table to add an object or test for the presence
of an object in the set. In our variant, we use multiple
prime-number-sized hash tables with each having a different hash
function corresponding to the modulus of the DNA bitstring
representation with the hash table size; the underlying properties of
the Bloom filter are identical, however.  To add a k-mer to the
filter, the corresponding bit is set to 1 in each hash table.  To find
the presence of a k-mer, each table is queried for the presence of
that k-mer; for a k-mer to be considered present in the dataset, the
k-mer must be found in all of the hash tables.  If a k-mer is not
present in any one hash table, then it is certainly not in the
dataset. As a result, there is a one-sided error: false positives are
possible but false negatives are not. The expected false positive rate
is simply the product of the occupancies of the hash tables.  As with
other hash-style data structures, Bloom filters have a fast lookup
time, $O(h)$ for $h$ hash tables.  Similar to other hash-style data
structures for storing k-mers, memory usage is independent of the
value of $k$.

\subsection{Calculating Data Structure Properties}
The properties of our Bloom filter variant are essentially the
same as a classical Bloom filter \cite{bloomsurvey}.
For example, to calculate the expected false positive 
rate, we
take the product of the occupancy of each hash table:
\begin{displaymath}
P_f = \prod_{h \in H} occ(h)
\end{displaymath}
where $h$ is a hash table in the set of hash tables $H$ and $occ$ denotes
the occupancy (proportion of bits set) for a hash table.
We can find the optimal number of hash tables
to use by calculating
\begin{displaymath}
\vert H \vert_{opt} = \ln 2 \frac{m}{k}
\end{displaymath}
where $m$ is the amount of memory in bits to allocate and $k$
is the number of unique k-mers to be inserted. Finally,
the number of bits per
k-mer used in the data structure for the optimal number of hash 
tables is

\begin{displaymath}
\frac{m}{n} = \frac{1}{\ln(2)} \log{\frac{1}{P_f}}.
\end{displaymath}

\begin{figure}
\center{\begin{tabular}{ | c | c | }
\hline
$f_p$ & Bits/k-mer \\ \hline
0.1 \% & 14.35 \\ \hline1 \% & 9.54 \\ \hline
5 \% & 6.22 \\ \hline
10 \% & 4.78 \\ \hline
15 \% & 3.94 \\ \hline20 \% & 3.34 \\
\hline\end{tabular}}
\caption{The number of bits needed to store each k-mer for selected
false positive rates.}
\end{figure}

\subsection{Using The Bloom Filter As A K-mer Graph}
% @CTB FIGURE
Having stored the k-mers in a Bloom filter, we can traverse
the data as a k-mer graph. We let each k-mer be a vertex, where
the reverse complement of a k-mer is considered the same
vertex. Each k-mer can
have up to eight neighbors, which are any of the other k-mers that
 share a $k-1$
overlap; no explicit edge is stored. In doing so, we implicitly 
treat the graph as a simple graph as opposed to a multigraph or 
digraph, which means that there can be no self-loops or parallel 
edges between vertices/k-mers. This graph representation is constant 
in its memory usage, so only ancillary information such as list 
of vertices visited or waypoints in the graph consumes additional 
memory.

This graph structure is effectively {\em compressible}
because one can choose a larger
or smaller size for the underlying Bloom filters; a larger size admits fewer
false positives, while a smaller size admits more. By relying on Bloom
filters, the data structure is constant memory: no extra memory is
used as additional data is added. However, as memory is decreased or data
is increased, only false positive vertices and edges are gained, so
compressing the graph results in a more tightly interconnected graph.

In contrast to an exact approach, there is a chance that a k-mer 
will be adjacent to a false positive,
that is, a k-mer
that does not actually exist in the original dataset but registers as present  
due to 
the Bloom filter. If this probability is too high, the 
graph can become dominated by false connectivity. 
Intuitively, it is clear that when most real k-mers
have at least one erroneous neighbor ($p_f \approx \frac{1}{8}$), 
false paths between real (but unconnected) k-mers are likely to 
appear. However, it is not immediately clear at exactly what 
false positive rate this occurs. We address this below.  % clumsy, I know.

\subsection{Estimating False Positive Rate For Erroneous Connectivity}
We ran a simulation to find when connected components in the graph 
begin to erroneously connect to one another.
To calculate the false positive rate $p$ at which this aberrant 
connectivity occurs, 
we added random k-mers, sampled from a uniform GC distribution, to the data structure 
and then calculated the occupancy and size of 
the largest connected 
component. From this we sampled the relative size of 
the largest component and the overall cluster size distribution for each
given occupancy rate.
At the occupancy where a ``giant cluster'' appears, this cluster size distribution 
must be scale-free \cite{stauffer1979scaling}. 

We then found at what value of $p$ the resulting 
cluster size distribution in logarithmic 
scale can be better fitted in a linear or quadratic fashion using 
the F-statistic
\newline
\newline
\begin{displaymath}
F=\frac{RSS_1-RSS_2}{p_2-p_1} \times \frac{n - p_2}{RSS_2}
\end{displaymath}

where $RSS_i$ is the residual sum of squares for model $i$, $p_i$ is 
the number of parameters for model $i$, and $n$ is the number of data 
points. To handle the finite size sampling error, the data was binned using the 
threshold binning method \cite{adami2002critical}. The critical value for 
when aberrant connectivity occurred was found by determining the local maxima 
of the F-values.

\subsection{Graph Partitioning Using A Bloom Filter}
We used the Bloom filter data structure containing the k-mers from a
dataset to discover disconnected components of the graph, i.e. to
partition the graph.  Here a connected component is a set of k-mers
whose originating reads overlap transitively by at least $k$ base
pairs.  To partition the reads, we tag the graph at a minimum density
by using the underlying reads as a guide. We then exhaustively explore
the graph around these tags in order to connect tagged k-mers based on
graph connectivity.  The underlying reads in each connected component
can then be separated and analyzed individually without affecting the
overall result, because they do connect outside that read set.

% @CTB mention k_0 > k_1 etc.

\subsection{Software and Software Availability}

We have implemented this compressible graph representation in a
software package named khmer.  It is written in C++ and Python 2.6 and
is available under the BSD open source license at
https://github.com/ctb/khmer.  The graphviz software package was used
for graph visualizations. The scripts to generate the figures of this
paper are available in the khmer repository.

\section{Results}

\subsection{False positives cause local elaboration of graph structure}

We wanted to understand how erroneous neighbors created by false
positives can alter the local graph structure, so we 
generated a random 1,031bp circular sequence to visualize the effect of false
positives.  After storing this sequence in compressible graphs using
$k=31$ with four different false positive rates ($p_f$=0.01, 0.05,
0.10, and 0.15), we explored the graph using breadth-first search
beginning at the first 31-mer.  The graphs in Figure 2 demonstrate how
the local graph structure elaborates with the false positive rate
while the overall circular graph structure remains, with no erroneous
shortcuts between k-mers that are present in the original
sequence. Because long erroneous paths are unlikely to occur, a
relatively high false positive rate ($\sim$ 15\%) is unlikely to
connect components together erroneously. Note that when the false
positive rate is sufficiently high, the resulting graph is
(practically speaking) impossible to visualize due to the effect of
erroneous k-mers connecting to other erroneous k-mers.

% @CTB I thought percolation threshold was independent of k?  Why does
% large k affect anything??

\begin{figure}
\includegraphics[width=3in]{figures/f3b001}
\includegraphics[width=3in]{figures/f3b005}\\
\includegraphics[width=3in]{figures/f3b010}
\includegraphics[width=3in]{figures/f3b015}
\caption{Graph visualizations demonstrating the decreasing 
fidelity of graph structure with increasing false positive rate. From 
top left to bottom right, the false positive rates are 0.01, 0.05, 0.10, 
and 0.15.}
\end{figure}

In addition, it is simple to see that a linear increase in the false 
positive rate results in a linear increase in the number of expected 
neighbors for a particular k-mer. For most isolated k-mers (i.e. no adjacent 
``real'' k-mers), the calculation is 
E(erroneous neighbors)$ = 8 \times p_f$. Thus, the local graph 
structure breaks down in a linear fashion. However, this offers no insight 
to how the global graph structure degrades.

\subsection{False long-range connectivity is a nonlinear function of the false positive rate}
We wanted to explore the point at which our data structure systematically 
engenders false long-range connections, 
so we randomly inserted 31-mers into Bloom
filters with increasing false positive rates and calculated the average
cluster size for each false positive rate. Figure 3 demonstrates that 
the average cluster
size rapidly increases as a specific threshold is approached,
which appears to be at a FP rate near 0.18 for k=31. Beyond 0.18, 
the connected components begin to join together as a single giant 
cluster, and it is no longer possible to distinguish between reads 
that do not overlap in the original dataset using the Bloom filter.

\begin{figure}
\center{\includegraphics[width=5in]{newclustersize}}
\caption{Average cluster size versus false positive rate. The average 
cluster size sharply increases as the false positive 
rate approaches the percolation threshold.
}
\end{figure}

As the false positive rate increases, there appears to be a sudden
transition from where the graph is comprised of disconnected 
components to where they begin to erroneously connect (Figure 3). 
In contrast to the linear-style change seen 
in local graph structure as the false positive rate increases linearly, 
the change in global graph structure is abrupt as previously disconnected 
clusters join together.  
This rapid change appears to resemble a phase transition, which for graphs
can be 
discussed in terms of percolation theory. We can map 
our problem to site percolation by considering a probability $p$ that a 
particular k-mer is the ``on'' state. This is in contrast to bond percolation where 
$p$ represents the probability of a particular edge being in the ``on'' state. As
long as the false positive rate is below the percolation threshold $p_\theta$ (in
the subcritical phase), it is feasible to traverse the graph. If the
false positive rate is at or above this (in the supercritical phase), then graph
traversal is unpractical.

% @CTB MENTION ITERATIVE PARTITIONING

The percolation thresholds for finite graphs can be estimated by
finding where the cluster size distribution transitions from linear to
quadratic in form.  Using the calculation method described
in \emph{Methods}, we found that the site percolation threshold for
DNA k-mer graphs is $p_\theta = 0.183 \pm 0.001$.  The percolation
threshold appears to be the same for different $k$, which suggests
that it is independent of $k$. This implies that as long as
the false positive rate is below $0.183$, connected components in the
graph are unlikely to erroneously connect to one another.

\subsection{Large-scale graph structure is retained to the percolation threshold}
The results from cluster size analysis and the percolation threshold 
estimation suggest that 
global connectivity of the graph is unlikely 
to change below the percolation threhold. To demonstrate this, we employed 
the diameter metric in graph theory.  
The diameter of a connected component in a graph is a measure of 
the length of the ``longest shortest'' 
path between any two vertices\cite{bondy2008graph}.
In our case, we only considered paths between two real k-mers
in the dataset for the only component that contains real k-mers. 
We randomly generated 58bp long circular
chromosomes to construct components containing 50 k-mers (setting $k=8$) and 
calculated the diameter at different false positive rates, averaging
the results from multiple runs ($n=10$). For each false positive rate, we 
repeated the 10 runs an additional 200 times to obtain 2.5 and 97.5 
percentiles. 
As Figure 4 shows, 
erroneous connections between pairs of ``real'' k-mers are unlikely
below the 
percolation threshold. At or after the percolation threshold, spurious connections 
between real k-mers are created, which lowers the diameter. 
Thus, the larger scale graph structure is retained until $p=0.183$, as suggested by the cluster size analysis and percolation results.

\begin{figure}
\center{\includegraphics[width=5in]{figures/newdiam}}
\caption{Length of Longest Shortest Path by False Positive Rate. The 
length of the longest shortest path of randomly generated 58bp 
long circular chromosomes in 8-mer 
space for different false positive rates. Only real (non-error) k-mers are
considered for starting and ending points.}
\end{figure}

\subsection{Sequencing errors eclipse false positives}
One important consideration when determining the usefulness of our
k-mer graph representation is how it compares to graphs built from
real data from massively parallel sequencers such as Illumina, which
contains base calling errors.  In de Bruijn graph-based assemblers,
sequencing errors add to the graph complexity and make it more
difficult to find high-quality paths for generating long, accurate
contigs. Since our approach generates false positives, we wanted to
compare the error rate from the Bloom filter graph with experimental
errors generated by sequencing (Figure 5). We used an
\emph{E. coli} K-12 MG1665 genome to compare various graph invariants
between an Illumina dataset of the same strain (see \emph{Methods}),
an exact representation of the genome, and inexact representations
with different false positive rates. Additionally, we used the linear 
model available 
from the Velvet mailing list\footnote{http://listserver.ebi.ac.uk/pipermail/velvet-users/2009-July/000474.html} to find that Velvet needs $\sim 876$ MB 
to assemble the dataset that we used.

\begin{figure}
\center{\begin{tabular}{ | c || c | c | c | c | c | c |}
\hline
Data + FPR & No. K-mers & No. Add'l & No. Missing & Deg $\ge 2$ & \% Real & Mem (bits) \\ \hline \hline
\emph{E. coli} 0 \% & 4,530,123 & 0 & 0 & 50,605 & 100 & $1.71 \times 10^{10}$ \\ \hline
\emph{E. coli} 1 \% & 4,814,050 & 283,927 & 0 & 313,844 & 94.1 & $4.34 \times 10^7$ \\ \hline
\emph{E. coli} 5 \% & 6,349,301 & 1,819,178 & 0 & 1,339,102 & 71.3 & $2.82 \times 10^7$ \\ \hline
\emph{E. coli} 15 \% & 31,109,523 & 26,579,400 & 0 & 10,522,482 & 14.6 & $1.79 \times 10^7$ \\ \hline
Reads 0 \% & 45,566,033 & 41,036,029 & 119 & 7,700,483 & 9.9 & $1.71 \times 10^{10}$ \\ \hline
Reads 1 \% & 48,237,038 & 43,707,032 & 117 & 9,771,028 & 9.4 & $4.37 \times 10^8$ \\ \hline
Reads 5 \% & 62,094,757 & 57,564,749 & 115 & 18,116,934 & 7.3 & $2.84 \times 10^8$ \\ \hline
Reads 15 \% & 235,654,877 & 231,124,854 & 100 & 85,813,621 & 1.9 & $1.8 \times 10^8$ \\
\hline
\end{tabular}}
\caption{An exact \emph{E. coli} genome representation of 17-mers was compared with 
two inexact ones as well as an exact and two inexact representations of an Illumina 
\emph{E. coli} dataset.}
\end{figure}

For these comparisons, we used a $k$ value of 17, which allows for exact
representation in memory. We found 
that there were 50,605 17-mers in the exact representation that were no
part of a simple line, i.e. had more 
than two neighbors (degree $>$ 2). As the false positive rate 
increased, the number of these 
17-mers increased in the expected linear fashion in addition to the number of 
false positive 17-mers found in each connected component. Furthermore, the number of 
``real'' 17-mers, those that are not false positives, 
comprise the majority of the graph.

In contrast, when we examined an exact representation of an Illumina
dataset, only 9.9\% of the k-mers in the graph truly exist in the
reference genome. Note that we only counted false positive k-mers that
are transitively connected to least one real k-mer. The
number of 17-mers with more than 2 neighbors is higher
than for the exact representation of the genome, which demonstrates
that sequencing errors add to the complexity of the
graph. Thus, we find that the errors demonstrated by sequencers dwarf
the errors caused by the inexact graph representation below a
reasonable false positive rate.

\subsection{Sequences can be accurately partitioned by graph connectivity}

Can we use this lightweight graph representation to find disconnected
components in de Bruijn graphs?
The diameter results suggest that unconnected
components are unlikely to connect below the percolation threshold. To
show this, we broke up the \emph{E. coli} genome at k-mers with a
degree greater than two and compared it with a simulated dataset of
1,000 randomly generated contigs containing 10,000bp each. Using
$k=32$, we partitioned the datasets using the procedure described in
\emph{Methods} at different false positive rates (Figure 6). As
expected, the resulting number of partitions did not change for the
simulated dataset. For the \emph{E. coli} dataset, the number of
partitions drops slowly with increasing false positives, which is
likely due to components that are close but were forcibly disconnected
in our simulated data set.
% @CTB repartitioning

\begin{figure}
\center{\includegraphics[width=5in]{figures/partplot}}
\caption{Number of Partitions by False Positive Rate. The 
graph shows the resulting number of partitions for the 
\emph{E. coli} genome broken up in high-complexity regions (blue) and 
a simulated dataset with 1,000 contigs of 10,000 bp each (red).}
\end{figure}

\section{Discussion}

\subsection{Bloom filters can be used to efficiently store de Bruijn graphs}


The use of Bloom filters to store an implicit de Bruijn graph is
straightforward and quite memory efficient.  The expected false
positive can be tuned based on hash table occupancy, and yields a wide
range of possible storage efficiences.

% @@ show table with bits etc.

Even for low false positive rates such as 5\%, this is still a very
efficient graph representation, with an observed 20-fold improvement
in memory usage over one existing assembler, Velvet.  The false
positive rates have a small effect on graph structure compared to
sequencing errors, suggesting that this is an effective representation
in practice as well.  Note that the false positives engendered by the
Bloom filters are uncorrelated with the original sequence, unlike
sequencing errors which usually have a low edit distance to the
real sequence; this may further reduce the effect of false positives
on de Bruijn graph analysis, depending on the application.

\subsection{Preservation of long-range structure permits graph partitioning}

Using Bloom filters raises the spectre of systematic graph artifacts
resulting from the false positives.  For partitioning, one concern is
that false positives will incorrectly connect components, rendering
partitioning ineffective.  Our results from percolation analysis,
diameter calculations, and partitioning of simulated data demonstrate
that below the calculated percolation threshold this is not a
significant problem.  As long as the expected rate of false edges is
below one per node, long false paths are not spontaneously created and
hence the graph is falsely connected.  While the \emph{E. coli} partitioning
does show some false connectivity, this is probably due to the
unrealistic way in which we broke up the \emph{E. coli} data; in reality, we
would expect disconnected components to be separated by more than one
node.  Regardless, falsely connected partitions can easily be broken
up by iterative partitioning, in which partitions resulting from the
first round are individually partitioned again.

Combined with the scaling properties of the graph representation,
partitioning offers a way to efficiently apply a "divide and conquer"
strategy to assembly problems.  The next challenge is to develop an
efficient way for finding breakpoints, or equivalently to characterize
minimally connected components; existing graph algorithms for
e.g. "betweenness centrality" do not scale to graphs with millions or
billions of nodes.

There are several biological problems that particularly well suited to
a partitioning approach.  Unlike genomes, where the assembly graph is
usually entirely connected due to repeats, transcriptomes and
metagenomes consist of populations of largely {\em disconnected}
sequences.  This has been exploited for assembly improvement.
Trinity, for example, relies on a partitioning approach in the second
phase (XXX) of transcriptome assembly; and both meta-idb and
metavelvet rely on locality of connections for metagenome assembly.
These algorithms, however, rely on exact graph representations that are
heavyweight compared to ours. @@

\subsection{Concluding thoughts}

The probabilistic de Bruijn graph representation presented here has a
number of convenient features.  First, it is compressible: for a given
data set, a wide range of false positive rates can be chosen without
impacting the global structure of the graph, allowing graph storage in
as little as 6 bits per k-mer.  Because a higher false positive rate
yields a more elaborate local structure, this allows memory to be
traded for traversal time in e.g. partitioning.  Second, it is
constant memory, with predictable degradation of both local and global
structure as more data is inserted.  For data sets where the number of
unique k-mers is not known in advance, the occupancy of the data
structure can be monitored as data is inserted and directly converted
to an expected false positive rate.  Third, the memory usage is
independent of the wordsize chosen, making this representation
convenient for exploring properties at many different parameters.  It
even allows the storage and traversal of k-mer graphs at multiple word
sizes within a single structure, although we have not explored these
properties here.

Our initial motivation for this data structure was to explore
partitioning as an approach to scaling metagenome and transcriptome
assembly, but there should be additional uses beyond partitioning. In
general a more memory efficient de Bruijn graph representation may
open up many opportunites.  While de Bruijn graph approaches are being
used primarily for the purposes of assembly, they are a broadly useful
formalism for sequence analysis. In particular, efforts have been made
to extend them to multiple-sequence alignment\cite{zhang2003dna},
repeat discovery\cite{price2005novo}, detection of local variation and
structural variant detection (zerbino etc.).  We and collaborators are
also exploring the use of de Bruijn graphs for graph-based homology
search and artifact detection in large sequencing data sets.  While de
Bruijn graphs have traditionally been viewed as heavy weight
computational solutions due to the extremely large memory
requirements, our initial investigations into the PDBG described here
demonstrate a $\approx$ 20-fold decrease in memory requirements over
XXX (Velvet).  This may translate into similar memory savings in these
other applications.  One significant caveat to this reduced memory
usage is that as only the graph structure itself is stored in the
Bloom filters, additional information such as node weights and
traversal waypoints will take up additional memory.  These questions
remain to be explored in future work.

\subsection{Acknowledgements}

Jim Cole and Jordan Fish.  Qingpeng.  Adina.  Adami.  JGI folk.

\bibliographystyle{abbrv}
\bibliography{kmer-percolation}
\end{document}


