% NOTES:
%%    do we want to explore for larger S?  Does false positive rate change?

% review: first round, chris, enbody, cole/jordan?
% second round yanni, curt, james foster, abdol.

% TEST

\documentclass[12pt]{article} \usepackage{simplemargins}
\usepackage[pdftex]{graphicx} \graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

\begin{document}

\title{A probabilistic framework for compressible assembly graphs}
\author{Jason Pell, Arend Hintze, Rosangela Canino-Koning, C. Titus Brown}

\maketitle

\section{Abstract}

We introduce a fast and lightweight representation for DNA k-mer graphs
based on Bloom filters, a constant-memory probabilistic data structure that allows us
to compress graphs.  This data structure has a one-sided
error that is precisely tunable and yields predictable degradation of
graph properties as memory usage is decreased. We show degradation of global 
graph connectivity with an increasing false
positive rate relates to percolation theory. Through 
simulation, we find that percolation for random graphs occurs at a false positive rate 
of $0.183 \pm 0.001$, independent of k. We discuss ways that this
graph representation can be used to scale de novo DNA sequence assembly by characterizing the structure of
large assembly graphs.

\section{Introduction}

de novo Assembly approaches are key in many areas of biology.  Scaling
assembly to the current generation of sequencing technologies is
difficult.  Given the required depth of sampling for transcriptome and
metagenome sequencing, we need to (continue to) scale.  This must be
done with improved approaches rather than hardware.

de Bruijn graph assembly is one of the dominant 
approaches\cite{trinity, panda, pmid21273488, pmid20203603}. 
It cannot meet the scaling challenges, which
are: (1) errors and (2) repeats.  

If we could analyze the overall graph structure, it would be possible
to develop approaches to scaling assembly by partitioning the graph or
removing highly connected components.  However, since the current
block to scaling is the memory required to hold the graph structure,
this is a no go.

We present a compressible representation for de Bruijn graphs based on
Bloom filters\cite{bloom}, a probabilistic data structure for tracking set
membership.  With this representation we can systematically trade
memory consumption for false positive edges and vertices in the graph.
While local graph properties such as degree are directly affected by
this false positive rate, measures of *global* graph connectivity such
as diameter do not change until the false positive rate reaches a
critical threshold.  The behavior of the graph at the critical
threshold resembles a geometric phase transition, suggesting that
accurate long range graph connectivity is retained until this point.
We can therefore use this compressible representation to efficiently
analyze assembly graph structure.  Moreover, this graph representation
possesses a number of useful properties, including constant memory
usage that is independent of the word size $k$, easy calculation of
optimal memory usage, and direct calculation of false positive rates.

This compressible graph representation permits us to efficiently store
and traverse large assembly graphs in memory, with an approximately
18x improvement in scaling over Velvet for real sequencing data.  We
demonstrate its use for efficiently partitioning simulated data and
discuss other potential uses, which include repeat discovery,
graph-based error correction, and graph search.

Bloom filters have been used for 
Bioinformatics applications 
in the past\cite{pmid20472541, haskell, pmid20426693} but not for graph 
storage.

\section{Methods}

\subsection{Data Structure Implementation}
% @JAP I added and merged some stuff, but I think the language needs to be 
% more ``direct''
We implemented a variation of the Bloom filter data structure to store k-mers
in memory. In a classic Bloom filter, multiple hash functions map into 
a single hash table to add an object or test for the presence of an object 
in the set. In 
our variant, we use multiple hash tables with each having a different hash function. 
To add a k-mer to the filter, the corresponding bit is set to 1 
in each hash table.  
To find the presence of a k-mer, each table is queried for the
presence of that k-mer; for a k-mer to be considered 
present in the dataset, 
the k-mer must be found in all of the hash tables. On the other hand, 
if a k-mer is not present in any one hash table, then it is certainly 
not in the dataset. As a result, there is a one-sided error where 
false positives are possible but false negatives are not. As with other 
hash-style data structures, Bloom filters have a
fast lookup time, which is $O(h)$ when there are $h$ hash tables to query
for k-mer presence. Similar to other hash-style data structures for 
storing k-mers, memory usage depends on the number of items inserted 
and is independent of the value of $k$. 

\subsection{Calculating Data Structure Properties}
The properties of our Bloom filter variant, which
uses a separate hash table for each hash function, are essentially the
same as a classical Bloom filter. For an example of how various 
properties of Bloom filters can be derived, see \cite{fan2000summary}. 
To calculate the expected false positive 
rate, we
take the product of the occupancy of each hash table; that is,
\begin{displaymath}
P_f = \prod_{h \in H} occ(h)
\end{displaymath}
where $h$ is a hash table in the set of hash tables $H$, $occ$ denotes
the occupancy (proportion of bits set) for a hash table.
We find the optimal number of hash tables
to use by calculating
\begin{displaymath}
\vert H \vert_{opt} = \ln 2 \frac{m}{k}
\end{displaymath}
where $m$ is the amount of memory in bits to allocate and $k$
is the number of k-mers to be inserted. Finally,
the number of bits per
k-mer used in the data structure for the optimal number of hash 
tables is

\begin{displaymath}
\frac{m}{n} = \frac{1}{\ln(2)} \log{\frac{1}{P_f}}.
\end{displaymath}
% @JAP re: references for this section
% this math is pretty straightforward to derive, but I have found a
% paper called Summary Cache: A Scalable Wide-Area Web Cache
% Sharing Protocol that does the relevant math if we want to use it

\subsection{Using The Bloom Filter As A K-mer Graph}
Having stored the k-mers in a Bloom filter, we can traverse
the data as a k-mer graph. We let each k-mer be a vertex, where
the reverse complement of a k-mer is considered the same
vertex. Each k-mer can
have up to eight neighbors, which are any of the other k-mers that
 share a $k-1$
overlap; no explicit edge is created. In doing so, we implicitly 
treat the graph as a simple graph as opposed to a multigraph or 
digraph, which means that there can be no self-loops or parallel 
edges between vertices/k-mers. This graph representation is constant 
in its memory usage, so only ancillary information such as list 
of vertices visited or waypoints in the graph consumes additional 
memory.

In contrast to an exact approach, there is a chance that a k-mer 
will be adjacent to a false positive,
that is, a k-mer
that does not actually exist in the original dataset but ``shows up'' 
due to the false
positive rate from the Bloom filter. If this probability is too high, the 
graph can become unusable since the false k-mers dominate the graph 
and erroneously connect components.
Intuitively, it is clear that when most real k-mers
have at least one erroneous neighbor ($p_f \approx \frac{1}{8}$), 
false paths between real (but unconnected) k-mers are likely to 
appear. However, it is not immediately clear at exactly what 
false positive rate this occurs. We address this below.  % clumsy, I know.

This graph structure is effectively compressible 
because one can choose a larger
or smaller size for the underlying Bloom filters; a larger size admits fewer
false positives, while a smaller size admits more. By relying on Bloom
filters, the data structure is constant memory: no extra memory is
used as additional data is added. However, as memory is decreased or data
is increased, only false positive vertices and edges are gained, so
compressing the graph results in a more tightly interconnected graph.

\subsection{Estimating False Positive Rate Of Erroneous Connectivity}
We ran a simulation to find when connected components in the graph 
begin to erroneously connect to one another.
To calculate the false positive rate at which this aberrant 
connectivity occurs, 
we added random k-mers to the data structure 
and then calculated the occupancy and size of 
the largest connected 
component. This allowed us to sample the relative size of 
the largest component for every 
given occupancy as well as the cluster size distribution. 
At the occupancy where a ``giant cluster'' appears, this cluster size distribution 
must be scale-free\cite{stauffer1979scaling}. 

We then found at what value of $p$ the resulting 
cluster size distribution in logarithmic 
scale can be better fitted in a linear or quadratic fashion using 
the F-statistic
\newline
\newline
\begin{displaymath}
F=\frac{\frac{RSS_1-RSS_2}{p_2-p_1}}{\frac{RSS_2}{n-p_2}}.
\end{displaymath}

To handle the finite size sampling error, the data was binned using the 
threshold binning method\cite{adami2002critical}. The critical value for 
when aberrant connectivity occurred was found by finding the local maxima 
of the F-values.

\subsection{Graph Partitioning Using A Bloom Filter}
We used the Bloom filter data structure containing the k-mers from a dataset 
to discover disconnected components of the graph. In a k-mer graph, a connected 
component whose k-mers originated from a set of reads represents a grouping of 
reads that overlap with one another by at least $k$ bases directly or transitively.
To partition the reads in such a fashion, we tag the 
graph at a minimum density by using the underlying reads as a guide. We then
explore the graph locally to connect tagged k-mers together and assign them 
to a partition ID. Finally, tags that are assigned to multiple partition IDs 
are systematically resolved to a single partition ID.  
When the underlying reads in each connected component are discovered, they can be 
separated and assembled individually to save overall memory requirements.

\subsection{Software and Software Availability}
We have implemented this compressible graph representation in a software package
named khmer.
It is written in C++
and Python 2.6 and is available under the BSD open source license at
https://github.com/ctb/khmer.
The graphviz software 
package was used for graph visualizations. The scripts to 
generate the figures of this paper are available in the khmer repository.

\section{Results}

\subsection{Effect Of False Positives on Local Graph Structure}
We wanted to understand how erroneous neighbors created 
by false positives can alter 
the local graph structure, so we randomly generated a 1,031bp circular sequence 
to visualize the effect of false positives.
After storing this sequence in compressible graphs using $k=31$ with
four different false positive rates ($p_f$=0.01, 0.05, 0.10, and
0.15), we explored the graph using breadth-first search beginning at
the first 31-mer. 
The graphs in Figure 
1 demonstrate how
the local graph structure begins to elaborate while the original circular
graph remains intact with no erroneous paths between k-mers that are
truly present in the originally generated sequence. Because the k-mer
space is extremely large for large k, a relatively high false positive 
rate ($\sim$ 15\%) is still 
unlikely to connect components
together erroneously. Note that when the false positive
rate is sufficiently high, the resulting graph is 
(practically speaking) impossible to visualize for high values of $k$ (e.g. $\ge 25$) 
due to the effect of erroneous k-mers connecting to other erroneous k-mers. 

\begin{figure}
\includegraphics[width=3in]{figures/f3b001}
\includegraphics[width=3in]{figures/f3b005}\\
\includegraphics[width=3in]{figures/f3b010}
\includegraphics[width=3in]{figures/f3b015}
\caption{Graph visualizations demonstrating the decreasing 
fidelity of graph structure with increasing false positive rate. From 
top left to bottom right, the false positive rates are 0.01, 0.05, 0.10, 
and 0.15.}
\end{figure}

In addition, it is simple to see that a linear increase in the false 
positive rate results in a linear increase in the number of expected 
neighbors for a particular k-mer. For most isolated (i.e. no adjacent 
``real'' k-mers) k-mers, the calculation is 
E(erroneous neighbors)$ = 8 \times p_f$. Thus, the local graph 
structure breaks down in a linear fashion. However, this offers no insight 
to how the global graph structure degrades.
% @JAP if it is the graph you are thinking of, I am not sure that I 
% ever incorporated it into the paper, unless you are thinking of something 
% Arend did. if it is that graph, then I didn't think to include it 
% because the linear effect seems to be intuitive

\subsection{Breakdown In Global Connectivity}
We wanted to explore the point where our data structure becomes unusable, 
so we randomly inserted 31-mers into Bloom
filters with increasing false positive rates and calculated the average
cluster size for each false positive rate. Figure 2 demonstrates that 
the average cluster
size rapidly increases as a specific threshold is approached,
which appears to be at a FP rate near 0.18 for k=31. Beyond 0.18, 
the connected components begin to join together as a single giant 
cluster, and it is no longer possible to distinguish between reads 
that do not overlap in the original dataset using the Bloom filter.

\begin{figure}
\center{\includegraphics[width=5in]{k31}}
\caption{Average cluster size versus false positive rate. The average 
cluster size sharply increases as the false positive 
rate approaches the percolation threshold.
}
\end{figure}

As the false positive rate increases, there appears to be a sudden
transition between the point where graph traversal is feasible (Figure 2). 
In contrast to the linear-style change seen 
in local graph structure as the false positive rate increases linearly, 
the change in global graph structure is abrupt as previously disconnected 
clusters join together.  
This rapid change appears to resemble a phase transition, which can be 
modelled using percolation theory. We can map 
our problem to site percolation by considering a probability $p$ that a 
particular k-mer is the ``on'' state. This is in contrast to bond percolation where 
$p$ represents the probability of a particular edge being in the ``on'' state. As
long as the false positive rate is below the percolation threshold $p_\theta$ (in
the subcritical phase), it is feasible to traverse the graph. If the
false positive rate is at or above this (in the supercritical phase), then graph
traversal is infeasible.

Using the calculation method described in \emph{Methods}, we found that the 
site percolation threshold for de Bruijn graphs is $p_\theta = 0.183 \pm 0.001$. 
The percolation threshold appears to be the same for 
different $k$, which suggests that the 
threshold is $k$ independent. This implies that as long as the 
false positive rate is below $0.183$, connected components in the graph 
are unlikely to erroneously connect to one another.

\subsection{Large-scale Graph Structure Retained to Percolation Threshold}
The results from cluster size analysis and the percolation threshold 
estimation suggest that 
global connectivity of the graph is unlikely 
to change below the percolation threhold. To demonstrate this, we employed 
the diameter metric in graph theory.  
The diameter of a connected component in a graph is a measure of 
the length of the ``longest shortest'' 
path between any two vertices\cite{bondy2008graph}.
In our case, we only considered paths between two real k-mers
in the dataset for the only component that contains real k-mers. 
We randomly generated 58bp long circular
chromosomes to construct components containing 50 k-mers (setting $k=8$) and 
calculated the diameter at different false positive rates, averaging
the results from multiple runs ($n=20$).
% I had to set k to be low because I wanted to go beyond the percolation
% threshold, and I couldn't think of a straightforward way to handle it
% for a high value of k...n=100 or whatever would be pretty easy...I just 
% wanted to get the graph generated
As Figure 3 shows, 
erroneous connections between pairs of ``real'' k-mers are unlikely
below the 
percolation threshold. At or after the percolation threshold, spurious connections 
between real k-mers are created, which lowers the diameter. For larger k, false connections 
between k-mers are even more unlikely since the shortest path between two k-mers 
with no overlap is $k$ in length.

\begin{figure}
\center{\includegraphics[width=5in]{figures/diameter}}
\caption{Length of Longest Shortest Path by False Positive Rate. The 
length of the longest shortest path of randomly generated 58bp 
long circular chromosomes in 8-mer 
space for different false positive rates. Only real (non-error) k-mers are
considered for starting and ending points.}
\end{figure}

\subsection{Sequencing Errors Eclipse Errors From Graph Representation}
One important consideration when determining the usefulness of our k-mer
graph representation is how it compares to errors from massively
parallel sequencers such as Illumina. In de Bruijn graph-based
assemblers, sequencing errors add to the graph complexity and make it
more difficult to find high-quality paths for generating long,
accurate contigs. Since our approach generates
false positives, we aim to show that sequencing errors dominate the graph complexity
issue. We used an \emph{E. coli} dataset provided by Illumina to compare
various graph invariants between the Illumina dataset, an exact
representation of the genome, and various inexact representations with
different false positive rates.

\begin{figure}
\begin{tabular}{ | c || c | c | c | c | c | }
\hline
 & No. K-mers & No. Additional & No. Missing & Deg $\ge 2$ & \% Real K-mers \\ \hline \hline
\emph{E. coli} 0 \% & 4530123 & 0 & 0 & 50605 & 100 \\ \hline
\emph{E. coli} 1 \% & 4814050 & 283927 & 0 & 313844 & 94.1 \\ \hline
\emph{E. coli} 5 \% & 6349301 & 1819178 & 0 & 1339102 & 71.3 \\ \hline
Reads 0 \% & 45566033 & 41036029 & 119 & 7700483 & 9.9 \\ \hline
Reads 1 \% & 48237038 & 43707032 & 117 & 9771028 & 9.4 \\ \hline
Reads 5 \% & 62094757 & 57564749 & 115 & 18116934 & 7.3 \\
\hline
\end{tabular}
\caption{An exact \emph{E. coli} genome representation of 17-mers was compared with 
two inexact ones as well as an exact and two inexact representations of an Illumina 
\emph{E. coli} dataset.}
\end{figure}

For these estimates, we used an \emph{E. coli} genome and an Illumina 
dataset of the same strain for the comparison using a $k$ value of 17. We found 
that there were 50,605 17-mers in the exact representation that had greater than 
2 degrees. As the false positive rate increased, the number of these 
17-mers increased in the expected linear fashion in addition to the number of 
false positive 17-mers found in each connected component. Furthermore, the number of 
``real'' 17-mers, those that are not false positives, 
comprise of the majority of the graph.

On the other hand, when we compared an exact representation of an Illumina dataset, 
only 9.9 \% of the k-mers in the graph truly exist in the genome. Note 
that we only counted false positive k-mers that are in the same component as 
at least one real k-mer. Furthermore, the number of 17-mers with more than 
2 neighbors is proportionally higher than for the exact representation of the 
genome, which demonstrates that sequencing errors greatly add to the complexity 
of the graph. Thus, we find that the errors demonstrated by 
sequencers dwarf the errors caused by the inexact graph representation 
below a reasonable false positive rate.

\subsection{Graph Representation Can Be Used To Partition Sequencing Data}
One advantage to the compressible graph is that it can be used to 
separate reads in a dataset to make the DNA sequence assembly less 
memory intensive. The diameter results suggest that unconnected components 
are unlikely to connect below the percolation threshold. To show this, we broke up the  
\emph{E. coli} genome at k-mers with a degree greater than two and compared it 
with a simulated dataset with 1,000 randomly generated contigs containing 
10,000bp each. Using 
$k=32$, we partitioned the datasets using the procedure described in 
\emph{Methods} for different false positive rates (Figure 5). As expected, the 
resulting number of partitions did not change for the simulated dataset. For the 
\emph{E. coli} dataset, the number of partitions drops slowly, which is likely 
due to components that were previously connected by a removed k-mer, but were 
then erroneously connected because the components are close together in the 
k-mer graph.

\begin{figure}
\center{\includegraphics[width=5in]{figures/partplot}}
\caption{Number of Partitions by False Positive Rate. The 
graph shows the resulting number of partitions for the 
\emph{E. coli} genome broken up in high-complexity regions (blue) and 
a simulated dataset with 1,000 contigs of 10,000 bp each (red).}
\end{figure}

\section{Discussion}

\subsection{Bloom Filters Can Be Used To Accurately Store Large Assembly Graphs}
The compressible graph representation we have built on a Bloom filter
is an efficient way to store and traverse k-mer graphs.  Per k-mer
memory usage is low compared to exact approaches without false positives,
and memory usage is independent of k.  K-mer vertex lookup and
local traversal are constant time.  The primary data structure can
also be implemented in constant memory.  This graph representation is
also easy to implement correctly.

The probabilistic nature of the data structure is a significant
concern, but the collision rate and resulting increase in false local
connectivity are predictable.  On a larger scale, we can link the
rate of increase in global connectivity from false positives to a
first-order phase transition, which lets us define a broad range of
parameters for which the global graph structure is extremely accurate.
Within this range, the primary effect of decreasing memory is to increase
traversal computation.  Note that this also provides a systematic way
to trade time spent in traversal (computation) off against memory. As shown 
in Figure 2, the amount of traversal from each k-mer increases 
(affecting computation) as the false positive rate increases, which 
is determined by the amount of memory that is allocated to the data 
structure.

The effect of increasing false positive rate as ``error'' can be
compared to the effect of sequencing errors.  Sequencing error
introduces both false negatives (by eliminating ``true'' k-mers from
low coverage samples) and false positives.  Novel k-mers
from sequencing errors also often result in k-mers that are
a low Hamming distance from the true k-mer, which can result in
elaborate graph structures.  In contrast, the Bloom graph error is
entirely one-sided, only resulting in false positives; moreover, these
false positives are uncorrelated with the ``true'' k-mers from which
they arise, and generally contribute to local graph structure only 
in a linear fashion as the false positive rate increases.

Overall, the Bloom k-mer graph is an efficient data structure for
storing and traversing large k-mer graphs.

\subsection{Decomposing Large Graphs}
Figure 5 shows that we can partition
genome connectivity graphs into perfectly disconnected subcomponents
given suitably chosen breakpoints.  Combined with the scaling
properties of the graph representation, this offers a way to
efficiently apply a "divide and conquer" strategy to assembly
problems.  The next challenge is to develop an efficient way for
finding breakpoints, or equivalently to characterize minimally
connected components; existing graph algorithms for e.g. "betweenness
centrality" do not scale to graphs with millions or billions of nodes.

While the assembly graph is entirely connected for most genomes --
either because there is a single chromosome, or because of repeat
elements that connect chromosomes -- we note that there are several
biological problems well suited to partitioning.  Both transcriptome
and metagenome sequencing target *populations* of disconnected
sequences.  Trinity, for example, relies on a partitioning approach in
the second phase (XXX) of transcriptome assembly; and both meta-idb
and metavelvet rely on locality of connections for metagenome
assembly.  However, these approaches do not address scaling.

\subsection{Other Uses}

There are many potential uses for a scalable de Bruijn graph representation.

Small component elimination.

Repeat structure characterization and filtering.

Graph search.

Graph-based error correction.

\subsection{Applying Bloom K-mer Graphs To DNA Sequence Assembly}
Bloom k-mer graphs may not be directly useful for traditional
approaches to DNA assembly, which rely on systematic heuristic
transformations of graph structure to find an optimal path through the
graph.  This is because the Bloom graph, as presented here, is limited
to representing k-mer graphs for a fixed k, and paths cannot readily
be compacted or eliminated.
% @CTB What did I mean ``for a fixed k''?  I don't know of any DBG
% reps that don't use a fixed k!

There are many uses, however, for an extremely scalable
graph representation.  Below we discuss the use of the
Bloom k-mer graph data structure as a vehicle for exploring graph
properties and filtering data sets.

For low-coverage samples, assembly graphs may contain many small
unconnected components, that represent unconnected sequences.
% @CTB why?  Beacuse of low coverage! Reiterate I guess.
Sequences contributing to these unconnected components can be safely
eliminated from the originating data sets without affecting the final
assembly.  This can be done efficiently with a simple limited depth
graph search algorithm. Since connected components in the graph 
are unlikely to erroneously connect when the false positive rate 
is below the percolation threshold, graph traversal is only affected 
by the minor changes to the local graph structure.

More generally, assembly graphs may contain many disconnected
subgraphs, due to the structure of the source data (e.g. transcriptome
or metagenome sequencing) or because of low coverage.  These subgraphs
can safely be partitioned into separate graphs without affecting the
final assembly, reducing the memory and computation required for
assembly of the whole to that required for the largest subgraph.
% @CTB discuss local vs global structure changes from FP on this.

The Bloom k-mer graph can also be used to do de novo repeat discovery
in collections of unassembled reads.  This in turn can be applied to
filtering of repeats prior to assembly\cite{hydra}, or for isolating
repeats from shallow or exploratory sequencing efforts.

It may also be possible to adapt sequence structure (e.g. ORF),
homology (BLAST), and domain search (HMMER) algorithms to search this
graph structure instead of searching either unassembled reads or
assemblies (Fish, Brown and Cole, pers communication).  Because 
assembly graphs implicitly collapse identical
sequences into a single path, this may be a more scalable approach to
targetted-gene analysis for metagenomics than current approaches
(which rely on searching individual reads that may be overlapping or identical).  Also note that, unlike
sequencing errors, the false positives in the Bloom graph will
generally bear no resemblance to biologically valid matches, which 
makes their detection during graph traversal easier.

The Bloom k-mer graph could also be used to develop connectivity-based
read trimming and correction algorithms.  For example, low-abundance
reads that contribute to ``spurs'' or ``sidings'' in otherwise
high-coverage regions could be corrected to match the
consensus, or trimmed to eliminate the divergent sequence.

One particularly intriguing option is to use the memory-efficient and
scalable Bloom k-mer graph representation as a component of a hybrid
assembler.  The k-mer graph approach could be used to identify
reads that belong to high-complexity regions, and extract them for
later resolution with more targetted approaches, e.g. an OLC assembler.
Alternatively, regions could be locally examined and extracted for
assembly, and then passed to scaffolding toolkit like Bambus for 
larger-scale connectivity\cite{bambus}.

\subsection{Dramatic Scaling}
It is clear that sequencing capacity will continue to outpace Moore's 
law and new methods must address this. Our graph representation allows 
for excellent scaling capabilities at the cost of a manageable false positive rate. 
As mentioned previously, $\sim 6.22$ bits per k-mer are required for a 
false positve rate of 5\%, so that modern computers with 
over 512GB of memory can handle graphs containing hundreds of billions of k-mers.

Another consideration is that assembly is generally difficult to parallelize. 
ABySS and Contrail (citation?), for example, place different parts of the 
graph on separate nodes. However, this can hinder performance due to 
network latency. Having the ability to examine the entire graph in memory 
before distributing the graph across multiple processors can minimize the
degree to which processors need to query one another. A smaller memory 
footprint can also mean that the entire assembly could be performed on 
a single node, if computational resources are limited.

Finally, though so-called ``third-generation'' sequencers are showing promise with 
longer reads and better error rates, we believe that improvements in these 
areas will not solve the scaling issue in certain domains. In particular, 
mRNAseq and metagenomics datasets are still sensitive to sampling depth as
well as read length. Differential expression detection and assembly 
of low-expressed genes are difficult without deep coverage. Thus
the required coverage level for an mRNAseq dataset is high, leading to large
data sets. Furthermore, many environmental samples with 
diverse microbial communities are estimated to require several terabases of sequencing 
in order to achieve a modest coverage level (citation?  my refs CTB). In both of these domains, better 
scaling algorithms are needed to handle these complex assemblies. 

\subsection{Concluding Remarks}
We have shown that representing a k-mer graph on a Bloom filter-like data 
structure can create an inexact, yet lightweight graph representation of 
a k-mer graph. This inexactness can be precisely known based on the occupancy 
of each hash table, and we demonstrated that our approach is still useful 
below a specific false positive rate. We conclude that while this representation 
may not be ideal for a final assembly, it can be useful for pre-filtering 
strategies and solving the scaling and memory bottleneck problems that we 
currently face in sequence assembly.

\subsection{Acknowledgements}

Jim Cole and Jordan Fish.  Qingpeng.  Adina.  Adami.  JGI folk.

\bibliographystyle{abbrv}
\bibliography{kmer-percolation}
\end{document}


